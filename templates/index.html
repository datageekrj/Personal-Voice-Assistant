<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Personal Voice Assistant</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            background-color: #f5f7fa;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            margin: 0;
        }
        
        .container {
            background-color: #ffffff;
            box-shadow: 0px 4px 12px rgba(0, 0, 0, 0.1);
            border-radius: 10px;
            width: 400px;
            text-align: center;
            padding: 20px;
        }

        h1 {
            color: #34495e;
        }

        #recognizedText {
            font-size: 18px;
            color: #2c3e50;
            background-color: #ecf0f1;
            padding: 10px;
            border-radius: 5px;
            min-height: 50px;
        }

        /* Microphone Icon with Animation */
        .microphone {
            margin: 20px auto;
            width: 100px; /* Increased size of the microphone */
            height: 100px;
            border-radius: 50%;
            background: linear-gradient(45deg, #3498db, #8e44ad, #f39c12, #e74c3c);
            background-size: 300% 300%;
            animation: gradientAnimation 3s ease infinite;
            display: flex;
            justify-content: center;
            align-items: center;
            position: relative;
            cursor: pointer; /* Cursor to indicate clickable */
        }

        /* Microphone Icon */
        .microphone svg {
            width: 50px; /* Increased size of the icon */
            height: 50px;
            fill: white;
        }

        /* Gradient Animation */
        @keyframes gradientAnimation {
            0% { background-position: 0% 50%; }
            50% { background-position: 100% 50%; }
            100% { background-position: 0% 50%; }
        }

        /* Pulse Animation for User Speaking */
        .user-speaking {
            animation: pulse 1s infinite ease-in-out;
        }

        @keyframes pulse {
            0% {
                transform: scale(1);
            }
            50% {
                transform: scale(1.1);
            }
            100% {
                transform: scale(1);
            }
        }

        /* Wave Animation for Bot Speaking */
        .bot-speaking {
            animation: wave 1.5s ease infinite;
        }

        @keyframes wave {
            0% { transform: rotate(0deg); }
            50% { transform: rotate(10deg); }
            100% { transform: rotate(0deg); }
        }

        /* Voice selection styling */
        #voiceSelect {
            margin-bottom: 20px;
            padding: 10px;
            width: 100%;
            border: 1px solid #ccc;
            border-radius: 5px;
        }

    </style>
</head>
<body>
    <div class="container">
        <h1>Personal Voice Assistant</h1>
        <!-- Voice Selection Dropdown -->
        <select id="voiceSelect">
            <option value="">Select a Voice</option>
        </select>

        <p id="recognizedText">Click the microphone to start speaking...</p>
        <div class="microphone" id="microphone">
            <!-- Microphone SVG -->
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 15c1.656 0 3-1.344 3-3v-6c0-1.656-1.344-3-3-3s-3 1.344-3 3v6c0 1.656 1.344 3 3 3zm7-3h-1c0 2.76-2.241 5-5 5s-5-2.24-5-5h-1c0 3.038 2.278 5.527 5 5.937v3.063h-3v2h8v-2h-3v-3.063c2.722-.41 5-2.899 5-5.937z"/></svg>
        </div>
        <audio id="audioPlayer" style="display: none;"></audio>
    </div>
    <script>
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();
        const synth = window.speechSynthesis;
        let voices = [];
        let selectedVoice = null;
        let history = []; // To maintain the conversation history
    
        const mic = document.getElementById('microphone');
        const voiceSelect = document.getElementById('voiceSelect');
        const audioPlayer = document.getElementById('audioPlayer');
    
        function populateVoiceList() {
            voices = synth.getVoices();
            voices.forEach((voice, index) => {
                const option = document.createElement('option');
                option.value = index;
                option.textContent = `${voice.name} (${voice.lang})`;
                voiceSelect.appendChild(option);
            });
        }
    
        speechSynthesis.onvoiceschanged = populateVoiceList;
    
        voiceSelect.addEventListener('change', () => {
            const selectedIndex = voiceSelect.value;
            selectedVoice = voices[selectedIndex];
        });
        function sanitizeText(text) {
            // Remove Markdown formatting symbols like * _ # and any other unwanted characters
            return text.replace(/[*_#`~>-]/g, '').trim();
        }
    
        function speak(text) {
            const sanitizedText = sanitizeText(text);
            if (synth.speaking) {
                synth.cancel();
            }
            const utterance = new SpeechSynthesisUtterance(sanitizedText);
            
            if (selectedVoice) {
                utterance.voice = selectedVoice;
            }
    
            synth.speak(utterance);
            mic.classList.remove('user-speaking');
            mic.classList.add('bot-speaking');
            utterance.onend = () => {
                mic.classList.remove('bot-speaking');
            };
        }
    
        async function getChatResponse(transcript) {
            try {
                // Send the user's input and history to the backend chat endpoint
                const response = await fetch('/chat', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        message: transcript,  // Send the recognized message
                        history: history      // Send the conversation history
                    })
                });
    
                const data = await response.json();
                history = data.history; // Update the conversation history
                return data.message; // Return the chatbot's response
            } catch (error) {
                console.error("Error fetching chat response:", error);
                return "I'm sorry, something went wrong.";
            }
        }
    
        recognition.lang = "en-US";
        recognition.interimResults = false;
        recognition.maxAlternatives = 1;
        recognition.continuous = true;
    
        recognition.onstart = () => {
            mic.classList.add('user-speaking');
        };
    
        recognition.onend = () => {
            mic.classList.remove('user-speaking');
        };
    
        recognition.onresult = async (event) => {
            const currentTranscript = event.results[event.resultIndex][0].transcript;
            console.log("User said:", currentTranscript);
            document.getElementById('recognizedText').textContent = currentTranscript;
    
            // Get the bot's response from the server
            const botReply = await getChatResponse(currentTranscript);
            
            // Call speak function with the bot's reply
            fetch('/process-text', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ text: botReply })
            })
            .then(response => response.blob()) // Get the audio data as a Blob
            .then(blob => {
                const audioUrl = URL.createObjectURL(blob);
                audioPlayer.src = audioUrl; // Set the Blob URL as the source for the audio element
                audioPlayer.play(); // Play the generated audio response
            })
            .catch(error => {
                console.error("Error processing text:", error);
            });
            // speak(botReply);
        };
    
        recognition.onerror = (event) => {
            console.error('Recognition error:', event.error);
        };
    
        mic.addEventListener('click', () => {
            recognition.start();
        });
    </script>    
</body>
</html>

